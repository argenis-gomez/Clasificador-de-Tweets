{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argenis-gomez/Clasificador-de-Tweets/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ6zhycx-LJa"
      },
      "source": [
        "# Fase 1: Importar las dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ9WTZWirSGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9128920-b00d-4f9c-f62c-bdf6d3718f7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7LmZ4UGjeBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efa37cf-c7d1-4955-8c6d-41907cd3f434"
      },
      "source": [
        "!pip install -q tensorflow_text_nightly\n",
        "!pip install -q tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 456.7MB 24kB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 58.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 27.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 50.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9MB 43.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 42.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 38.9MB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NON9giQ1_eZy"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8EilQG-cwa"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52CTV4_q-hpX"
      },
      "source": [
        "## Carga de Ficheros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGNRFfLC9tOF",
        "outputId": "4f17d68c-3e1a-486b-f9c8-f3f65ac1f782"
      },
      "source": [
        "tf.keras.utils.get_file(\"trainingandtestdata.zip\",\n",
        "                        \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\",\n",
        "                        cache_dir='.',\n",
        "                        cache_subdir='')\n",
        "\n",
        "with zipfile.ZipFile(\"trainingandtestdata.zip\", 'r') as file:\n",
        "  file.extractall(\"./\")\n",
        "\n",
        "os.rename(\"testdata.manual.2009.06.14.csv\", \"test.csv\")\n",
        "os.rename(\"training.1600000.processed.noemoticon.csv\", \"train.csv\")\n",
        "\n",
        "os.remove(\"trainingandtestdata.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "81371136/81363704 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Qkr_1Zfgi9"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "train_data = pd.read_csv(\n",
        "    \"train.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")\n",
        "test_data = pd.read_csv(\n",
        "    \"test.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DAwqYIO259aZ",
        "outputId": "ee5270b4-add1-46da-f610-b57db53474b1"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  ...  is upset that he can't update his Facebook by ...\n",
              "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3          0  ...    my whole body feels itchy and like its on fire \n",
              "4          0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIcAM2Qr2Z9B",
        "outputId": "21cd4aab-c2cf-4bfc-ca59-07d1f9120f3e"
      },
      "source": [
        "train_data.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Zrd3f0jm2JwU",
        "outputId": "db6ea7df-e791-431d-bd12-6ff5f0fc2090"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>tpryan</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>vcu451</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>chadfu</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>SIX15</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>yamarama</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  id  ...      user                                               text\n",
              "0          4   3  ...    tpryan  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
              "1          4   4  ...    vcu451  Reading my kindle2...  Love it... Lee childs i...\n",
              "2          4   5  ...    chadfu  Ok, first assesment of the #kindle2 ...it fuck...\n",
              "3          4   6  ...     SIX15  @kenburbary You'll love your Kindle2. I've had...\n",
              "4          4   7  ...  yamarama  @mikefish  Fair enough. But i have the Kindle2...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCmOlWV12QRa",
        "outputId": "991e9d3d-95dd-4be3-e7f7-7ae50d442438"
      },
      "source": [
        "test_data.sentiment.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Md2zOM-SPH"
      },
      "source": [
        "El conjunto de datos de testing tiene 3 etiquetas diferentes (una negativa, una positiva y una neutra), mientras que el conjunto de datos de entrenamiento tiene solo dos, por lo que no usaremos el archivo de testing y dividiremos el archivo de entrenamiento más tarde nosotros mismos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-6BBPb3-OfY"
      },
      "source": [
        "data = train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ESHUoWPDBFof",
        "outputId": "18c5a4e8-05f2-4776-f813-b1411c857689"
      },
      "source": [
        "data = data.sample(frac=1)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>240291</th>\n",
              "      <td>0</td>\n",
              "      <td>1980843418</td>\n",
              "      <td>Sun May 31 07:48:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>SaraahPcd</td>\n",
              "      <td>@pcdnicole your present 'cause your birthday i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266249</th>\n",
              "      <td>0</td>\n",
              "      <td>1988956829</td>\n",
              "      <td>Sun May 31 23:53:23 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jblockbuster</td>\n",
              "      <td>bought milk and coffee. Couldn't make foam wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331954</th>\n",
              "      <td>0</td>\n",
              "      <td>2012891451</td>\n",
              "      <td>Tue Jun 02 21:29:31 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tasherajean</td>\n",
              "      <td>@retrohandmade I finally found you guys.  I ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278294</th>\n",
              "      <td>0</td>\n",
              "      <td>1991495677</td>\n",
              "      <td>Mon Jun 01 07:08:34 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bibibubut</td>\n",
              "      <td>@DeniceSy :O DON'T TRUST ME ANYMORE, DEDE?  I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217802</th>\n",
              "      <td>0</td>\n",
              "      <td>1976014259</td>\n",
              "      <td>Sat May 30 16:24:05 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>lmarconi</td>\n",
              "      <td>Boston's Hot Dog Safari at Suffolk Downs is to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentiment  ...                                               text\n",
              "240291          0  ...  @pcdnicole your present 'cause your birthday i...\n",
              "266249          0  ...  bought milk and coffee. Couldn't make foam wit...\n",
              "331954          0  ...  @retrohandmade I finally found you guys.  I ke...\n",
              "278294          0  ...  @DeniceSy :O DON'T TRUST ME ANYMORE, DEDE?  I ...\n",
              "217802          0  ...  Boston's Hot Dog Safari at Suffolk Downs is to...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CsZKJx1-2Ep"
      },
      "source": [
        "## Pre Procesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLWayJ-5-7nN"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa1v91RSkgz1"
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU-7WW0m9O5j"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    tweet = re.sub(r\"www.[A-Za-z0-9./~]+\", ' ', tweet)\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbU2yZSqphgn",
        "outputId": "40d4f0c8-cf8b-4abb-b3fa-7a94c0e42094"
      },
      "source": [
        "%%time\n",
        "\n",
        "clean_data_path = '/content/drive/MyDrive/Clasificador Convolucional/clean_data.csv'\n",
        "\n",
        "if not os.path.isfile(clean_data_path):\n",
        "  \n",
        "  data_clean = [clean_tweet(tweet) for tweet in data.text]\n",
        "\n",
        "  data_labels = data.sentiment.values\n",
        "  data_labels[data_labels == 4] = 1\n",
        "\n",
        "  pd.DataFrame({'data': data_clean, 'labels': data_labels}).to_csv(clean_data_path, index=False)\n",
        "\n",
        "else:\n",
        "  print(\"Data limpia existente...\")\n",
        "  df = pd.read_csv(clean_data_path)\n",
        "  data_clean = df.data.values\n",
        "  data_labels = df.labels.values\n",
        "  print(\"Restaurada\", end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data limpia existente...\n",
            "Restaurada\n",
            "\n",
            "CPU times: user 2.02 s, sys: 171 ms, total: 2.19 s\n",
            "Wall time: 4.46 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4dApW03lbH5"
      },
      "source": [
        "### Vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12zYcLH5o8Mv"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TysbrJM9b6gQ"
      },
      "source": [
        "bert_tokenizer_params = dict()\n",
        "reserved_tokens = [\"[PAD]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    vocab_size = 2**13,\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    learn_params={},\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNoMHBGfwHF3",
        "outputId": "b47e341f-49d3-4481-a48b-6c1253b649ec"
      },
      "source": [
        "%%time\n",
        "\n",
        "vocab_path = '/content/drive/MyDrive/Clasificador Convolucional/vocab.txt'\n",
        "\n",
        "if not os.path.isfile(vocab_path):\n",
        "\n",
        "  vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "      tf.data.Dataset.from_tensor_slices(data_clean).batch(1000).prefetch(tf.data.experimental.AUTOTUNE),\n",
        "      **bert_vocab_args)\n",
        "\n",
        "  write_vocab_file(vocab_path, vocab)\n",
        "\n",
        "else:\n",
        "  print('Vocabulario existente...', end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulario existente...\n",
            "\n",
            "CPU times: user 1.2 ms, sys: 0 ns, total: 1.2 ms\n",
            "Wall time: 1.52 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTKZ5fUh_Kxz"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtJ0NmlJvimu"
      },
      "source": [
        "tokenizer = text.BertTokenizer(vocab_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCxUS1ojyQsd"
      },
      "source": [
        "data_inputs = tokenizer.tokenize(data_clean)\n",
        "data_inputs = data_inputs.merge_dims(-2,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYs5CPos3ziD"
      },
      "source": [
        "data_inputs = data_inputs.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysb2uib8n6b3"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9qttbt7BMwg"
      },
      "source": [
        "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
        "                                                            padding=\"post\",\n",
        "                                                            maxlen=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Ac7EXNNblp"
      },
      "source": [
        "### Dividimos en los conjuntos de entrenamiento y de testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynZDLlWbzCiA"
      },
      "source": [
        "split = int(len(data_inputs)*0.9)\n",
        "\n",
        "train_inputs = data_inputs[:split]\n",
        "train_labels = data_labels[:split]\n",
        "\n",
        "test_inputs = data_inputs[split:]\n",
        "test_labels = data_labels[split:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uxC5gvlKPIB"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels))\n",
        "train_ds = train_ds.shuffle(len(train_inputs)).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((test_inputs, test_labels))\n",
        "test_ds  = test_ds.shuffle(len(test_inputs)).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWu6hLDG_UJZ"
      },
      "source": [
        "# Fase 3: Construción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD3nbD_M94Gt"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_dim=128, nb_filters=50, FFN_units=512, dropout_rate=0.1, name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        self.dense = layers.Dense(units=FFN_units, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(2*dropout_rate))\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1)\n",
        "        merged = self.dense(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92XbAZ9E1AMS"
      },
      "source": [
        "# Paso 4: Aplicación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8cfYwHME-m0"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXwGD-pqFG4n"
      },
      "source": [
        "VOCAB_SIZE = 2**13\n",
        "\n",
        "EMB_DIM = 64\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "\n",
        "DROPOUT_RATE = 0.4\n",
        "\n",
        "NB_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nddzr1kA7UHC"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETcf5Wl4Q-7"
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "Dcnn.compile(optimizer=tf.keras.optimizers.Adam(2e-6, 0.5),\n",
        "             loss=tf.keras.losses.binary_crossentropy,\n",
        "             metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1X7h6Bx5Upc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9702fb-2a36-405d-b845-2bbae00128df"
      },
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/Clasificador Convolucional/ckpt'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Último checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxTavbiISHs5"
      },
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              patience=3,\n",
        "                                              restore_best_weights=True)\n",
        "\n",
        "class CheckPoint(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      ckpt_manager.save()\n",
        "        \n",
        "checkpoint = CheckPoint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21CE8okNRpRU",
        "outputId": "c954a91c-4377-44d7-e11c-7a216b352185"
      },
      "source": [
        "Dcnn.fit(train_ds,\n",
        "         validation_data=test_ds,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[early_stop, checkpoint])\n",
        "\n",
        "Dcnn.save_weights('/content/drive/MyDrive/Clasificador Convolucional/ckpt/model_weights/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1407/1407 [==============================] - 1802s 1s/step - loss: 0.3808 - accuracy: 0.8424 - val_loss: 0.4275 - val_accuracy: 0.8137\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 1839s 1s/step - loss: 0.3749 - accuracy: 0.8456 - val_loss: 0.4217 - val_accuracy: 0.8186\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 1839s 1s/step - loss: 0.3693 - accuracy: 0.8490 - val_loss: 0.4272 - val_accuracy: 0.8173\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 1832s 1s/step - loss: 0.3623 - accuracy: 0.8535 - val_loss: 0.4404 - val_accuracy: 0.8127\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 1801s 1s/step - loss: 0.3604 - accuracy: 0.8553 - val_loss: 0.4731 - val_accuracy: 0.8015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Gn6JhJKXDK"
      },
      "source": [
        "## Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_cFnXSMcy1X",
        "outputId": "dd1b26e3-1a32-4399-e984-2047782c8ea6"
      },
      "source": [
        "Dcnn.load_weights('/content/drive/MyDrive/Clasificador Convolucional/ckpt/model_weights/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4389136510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt2dRZWhKHbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5159d1f1-035c-4c66-dc92-46e76c3ab2b1"
      },
      "source": [
        "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 50s 321ms/step - loss: 0.4217 - accuracy: 0.8186\n",
            "[0.4216708838939667, 0.8185937404632568]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fizYYIXxbmkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9992b83d-b888-47a5-964f-8983ff5d0ae8"
      },
      "source": [
        "sentences = [\"You are so funny\",\n",
        "             \"i'm so happy\",\n",
        "             \"hate this sommer\",\n",
        "             \"This place is so boring\",\n",
        "             \"You are so beautifull\",\n",
        "             \"BA is the best place in world\",\n",
        "             \"I like you best\",\n",
        "             \"Espetial moment, bah! Was wonderfull, but i hate it.\"]\n",
        "\n",
        "max_sen = max([len(sentence) for sentence in sentences]) + 1\n",
        "\n",
        "sequence = tokenizer.tokenize(sentences)\n",
        "sequence = sequence.merge_dims(-2,-1)\n",
        "sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence.to_list(), padding=\"post\", maxlen=MAX_LEN)\n",
        "\n",
        "predictions = Dcnn(sequence, training=False).numpy()\n",
        "\n",
        "for twit, prediction in zip(sentences, predictions):\n",
        "  if prediction[0] > .5:\n",
        "    sentiment = 'positive'\n",
        "    value = prediction[0]*100\n",
        "  else:\n",
        "    sentiment = 'negative'\n",
        "    value = (1 - prediction[0])*100\n",
        "\n",
        "  print(f' - Tweet: {twit.ljust(max_sen, \" \")} --> Sentiment: {value:2.2f}% {sentiment}', end='\\n\\n')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Tweet: You are so funny                                      --> Sentiment: 98.94% positive\n",
            "\n",
            " - Tweet: i'm so happy                                          --> Sentiment: 98.95% positive\n",
            "\n",
            " - Tweet: hate this sommer                                      --> Sentiment: 82.66% negative\n",
            "\n",
            " - Tweet: This place is so boring                               --> Sentiment: 95.11% negative\n",
            "\n",
            " - Tweet: You are so beautifull                                 --> Sentiment: 98.39% positive\n",
            "\n",
            " - Tweet: BA is the best place in world                         --> Sentiment: 96.68% positive\n",
            "\n",
            " - Tweet: I like you best                                       --> Sentiment: 98.14% positive\n",
            "\n",
            " - Tweet: Espetial moment, bah! Was wonderfull, but i hate it.  --> Sentiment: 57.69% negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzZfZfe7-m8g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}